Your approach of using large language models (LLMs) purely as feature extractors is absolutely valid for comparing how well they capture semantics (in tasks like LaRoSeDa sentiment classification). Youâ€™re doing two main things:

Extract embeddings for each sample (by pooling the modelâ€™s hidden states).

Train a lightweight classifier (MLP) on top to see which LLMâ€™s features yield the best downstream performance.

Thatâ€™s perfectly fine for a research/benchmark project!

Below are a few practical suggestions and things to watch out for, specifically with LLaMA and other decoder-only (causal) models:

1. Using Causal LLMs as â€œSentence Embeddersâ€
Pros
You stay consistent across all models by using the same pooling logic.

It can work decently if you apply mean-pooling or more advanced â€œprompt engineeringâ€ to extract embeddings.

Cons
Unlike BERT-like encoders (which often have a dedicated [CLS] token), a causal LLM (e.g. LLaMA) is not explicitly trained to produce a single â€œsentence embedding.â€

The resulting vector might not be as specialized for classification tasks.

That said, your fair comparison approach is valid: each model is used the same way, so results are comparable.

2. Memory / Performance Gotchas
Quantization / Offloading
LLaMA-3.1 8B is huge for an 8GB GPU. You need:

torch_dtype=torch.float16 AND

load_in_4bit=True

Possibly device_map="auto"

This is in your code, so thatâ€™s good.

Watch system RAM usage if you see your process get killed.

Avoid Full FP32
Youâ€™ll almost certainly OOM with 8 GB VRAM.

Time / Speed
If youâ€™re extracting embeddings for thousands of samples, LLaMA-8B will be slow. Make sure to measure or at least factor this into your plan.

3. Pooling Strategies to Try
Your plan to compare last-token, mean pooling, prompt-based summarization is great. In practice, many LLM-based embedding methods do something like:

Mean Pooling:
mean_pooled
=
1
âˆ‘
mask
âˆ‘
(
â„
ğ‘–
ğ‘‘
ğ‘‘
ğ‘’
ğ‘›
_
ğ‘ 
ğ‘¡
ğ‘
ğ‘¡
ğ‘’
ğ‘ 
Ã—
ğ‘š
ğ‘
ğ‘ 
ğ‘˜
)
mean_pooled= 
âˆ‘mask
1
â€‹
 âˆ‘(hidden_statesÃ—mask)

Max Pooling:
max_pooled
=
max
â¡
(
â„
ğ‘–
ğ‘‘
ğ‘‘
ğ‘’
ğ‘›
_
ğ‘ 
ğ‘¡
ğ‘
ğ‘¡
ğ‘’
ğ‘ 
Ã—
ğ‘š
ğ‘
ğ‘ 
ğ‘˜
)
max_pooled=max(hidden_statesÃ—mask)

Prompt-based:
E.g. â€œThis text: {TEXT} can be summarized in one sentence: [LLM output].â€ Then embed that output. Or â€œIn one word, {TEXT} means: [LLM output].â€

The difference in performance can be surprising, so itâ€™s worth systematically testing them.

4. Specialized vs. General LLM Embeddings
Models like mGPT-1.3B or LLmic 3B might have had some Romanian data, but not necessarily as well-tuned for â€œuniversal sentence embeddings.â€ Thatâ€™s okay for your experiment. Just note that bigger model size â‰  automatically better embeddings.

If you ever need the best cross-lingual embeddings quickly, you could compare with a known encoder approach like [sentence-transformers or E5-models], but that might be out of scope if you specifically want to compare Romanian LLMs.

5. Implementation Details to Double-Check
Ensure each LLM is loaded with the same device/offloading strategy â€“ so youâ€™re comparing apples to apples.

Check hidden dimensionality. In your code, you assume the embedding is dim=2560 for every model. Make sure each LLMâ€™s hidden size actually matches that (or dynamically detect shape).

Save intermediate embeddings (which you already do in TitleContentDataset) to avoid re-running the LLM. Smart usage of save_interval is good.

6. Analysis & Visualization
T-SNE / UMAP on the embeddings is a great idea. It lets you visually see if positive/negative classes cluster in different regions.

Show not just accuracy but also F1 / confusion matrix if you have unbalanced classes.

Optionally measure inference time (seconds per sample) or GPU memory usage. Itâ€™s valuable to see if one model is huge but only marginally better.

7. Summary: Is Your Approach â€œGood Enoughâ€?
Yes, for a research experiment that compares different Romanian LLMs as feature extractors:

Youâ€™re consistent: same pooling method across all models.

Youâ€™re training the same MLP on top of those embeddings for classification.

Youâ€™re testing standard tasks (like LaRoSeDa sentiment).

Any changes?

Youâ€™re mostly good. Just ensure each big model is quantized/offloaded properly (like you did with load_in_4bit=True) to avoid out-of-memory.

Consider exploring a prompt-based embedding if you have time.

Make sure you handle each modelâ€™s embedding dimension carefully.

Final Take
Your design is solid for a comparative study. Keep going, and remember to:

Track memory/time usage.

Possibly try more advanced prompts for embedding.

Compare a smaller specialized model (like a sentence-transformers in Romanian, if available) as a baseline.

Good luck with the experiments!



--- Testing strategy: mean ---
Shapes: (4096,) (4096,)
Sample embedding1: [-0.6274  0.755   0.726   0.8345 -1.719 ] ...
Sample embedding2: [-1.88    0.9316  0.2666 -0.8735  1.232 ] ...

--- Testing strategy: max ---
Shapes: (4096,) (4096,)
Sample embedding1: [1.65625   2.4277344 2.6679688 3.5449219 3.4316406] ...
Sample embedding2: [1.6552734 3.7421875 3.6367188 3.5429688 8.28125  ] ...

--- Testing strategy: last_token ---
Shapes: (4096,) (4096,)
Sample embedding1: [-3.176   0.6333  2.668   0.2474 -2.807 ] ...
Sample embedding2: [-0.679  1.117  1.363 -2.387  1.536] ...